{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AWS Certified Developer (DVA-C02) Study Guide","text":"<p>This website contains the Study Guide put together while studying for the AWS Certified Developer (DVA-C02) exam.</p> <p>I basically put it in this format for several reasons:</p> <ul> <li>I found writing in markdown quite confortable</li> <li>I've experience using Material for MKDocs in several proyects at work</li> <li>Pusblish using Material for MKDocs to Github Pages is a breeze</li> <li>Having the source in Markdown I can also publish to ePub (using PanDoc) </li> <li>In ePub format is good I can read this Study Guide on my Kindle</li> <li>I want to have all my docs version controlled in Git</li> </ul> <p>GitHub Project (Source): https://github.com/jvanoli/dva-c02-study-guide</p> <p>Github Page (Published): https://jvanoli.github.io/dva-c02-study-guide/</p> <p>References:</p> <ul> <li> <p>Tutorials Dojo's Study Guide eBook \u2013 AWS Certified Developer Associate DVA-C02 https://portal.tutorialsdojo.com/product/tutorials-dojo-study-guide-ebook-aws-certified-developer-associate/</p> </li> <li> <p>Digital Cloud Trainings https://digitalcloud.training/</p> </li> </ul>"},{"location":"Exam%20Overview/","title":"Exam Overview","text":"<p>Amazon Web Services (AWS) began the Global Certification Program with the primary purpose of validating the technical skills and knowledge for building secure and reliable cloud-based applications using the AWS platform. By successfully passing the AWS exam, individuals can prove their AWS expertise to their current and future employers.</p> <p>The AWS Certified Solutions Architect - Associate exam was the first AWS certification that was launched in 2013, followed by two other role-based certifications: Systems Operations (SysOps) Administrator and Developer Associate later that year. AWS is always releasing new development services and features on its cloud platform; thus, there is a continuous stream of updates on its exam content. Some changes are minor ones, while others are major updates that overhaul the entire certification test.</p> <p>The very first version of the AWS Certified Developer - Associate exam (DVA-C00) and was released in January 2014. Four years later, in June 2018, AWS released the updated version of this test with an exam code of DVA-C01. AWS unveiled the third iteration of this test with DVA-C02 as its latest exam code. You might notice that for every major exam update, the last digit of the exam code is incremented, so expect that the next one will be called DVA-C03 after several years. It\u2019s quite important to know the relevant details of the AWS exam version to ensure that you are using up-to-date review materials</p>"},{"location":"Exam%20Overview/#exam-details","title":"Exam Details","text":"<p>The AWS Certified Developer - Associate examination is intended for individuals who perform a development role and have one or more years of hands-on experience developing and maintaining an AWS-based application.</p> Exam Code: DVA-C02 No. of Questions: 65 Score Range: 100/1000 Cost: 150 USD Passing Score: 720/1000 Time Limit: 2 hours 10 minutes (130 minutes) Format: Multiple choice/multiple answers Delivery Method: Testing center or online proctored exam"},{"location":"Exam%20Overview/#exam-domains","title":"Exam Domains","text":"<p>The AWS Certified Developer - Associate exam has four (4) different domains, each with corresponding weight and topic coverage. The exam domains are Development with AWS Services (32%), Security (26%), Deployment (24%), and Troubleshooting and Optimization (18%)</p> <ol> <li>Domain 1: Development with AWS Services (32%)<ol> <li>Develop code for applications hosted on AWS</li> <li>Develop code for AWS Lambda</li> <li>Use data stores in application development</li> </ol> </li> <li>Domain 2: Security (26%)<ol> <li>Implement authentication and/or authorization for applications and AWS services</li> <li>Implement encryption by using AWS services</li> <li>Manage sensitive data in application code.</li> </ol> </li> <li>Domain 3: Deployment (24%)<ol> <li>Prepare application artifacts to be deployed to AWS.</li> <li>Test applications in development environments </li> <li>Automate deployment testing</li> <li>Deploy code by using AWS CI/CD services.</li> </ol> </li> <li>Domain 4: Troubleshooting and Optimization (18%)<ol> <li>Assist in a root cause analysis</li> <li>Instrument code for observability.</li> <li>Optimize applications by using AWS services and features.</li> </ol> </li> </ol> <p></p>"},{"location":"Exam%20Overview/#exam-scoring","title":"Exam Scoring","text":"<p>You can get a score from 100 to 1,000 with a minimum passing score of 720 when you take the AWS Certified Developer Associate exam. AWS uses a scaled scoring model to associate scores across multiple exam types that may have different levels of difficulty. Your complete score report will be sent to you by email 1 - 5 business days after your exam. However, as soon as you finish your exam, you\u2019ll immediately see a pass or fail notification on the testing screen.</p> <p>For individuals who unfortunately do not pass their exams, you must wait 14 days before you are allowed to retake the exam. There is no hard limit on the number of attempts you can retake an exam. Once you pass, you\u2019ll receive various benefits, such as a discount coupon which you can use for your next AWS exam.</p> <p>Once you receive your score report via email, the result should also be saved in your AWS Certification account already. The score report contains a table of your performance on each domain and it will indicate whether you have met the level of competency required for these domains. Take note that you do not need to achieve competency in all domains for you to pass the exam. At the end of the report, there will be a score performance table that highlights your strengths and weaknesses, which will help you determine the areas you need to improve on.</p>"},{"location":"Exam%20Overview/#exam-benefits","title":"Exam Benefits","text":"<p>If you successfully pass any AWS exam, you will be eligible for the following benefits:</p> <ul> <li>Exam Discount - You\u2019ll get a 50% discount voucher to apply for your recertification or any other exam you plan to pursue. To access your discount voucher code, go to the \u201cBenefits\u201d section of your AWS Certification Account, and apply the voucher when you register for your next exam.</li> <li>AWS Certified Store - All AWS certified professionals would be given access to exclusive AWS Certified merchandise. You can get your store access from the \u201cBenefits\u201d section of your AWS Certification Account.</li> <li>Certification Digital Badges - You can showcase your achievements to your colleagues and employers with digital badges on your email signatures, Linkedin profile, or on your social media accounts. You can also show your Digital Badge to gain exclusive access to Certification Lounges at AWS re-invent, regional Appreciation Receptions, and select AWS Summit events. To view your badges, Go to the \u201cDigital Badges\u201d section of your AWS Certification Account.</li> <li>Eligibility to join AWS IQ - With the AWS IQ program, you can monetize your AWS skills online by providing hands-on assistance to customers around the globe. AWS IQ will help you stay sharp and well-versed in various AWS technologies. You can work in the comforts of your home and decide when or where you want to work.</li> </ul> <p>You can visit the official AWS Certification FAQ page to view the frequently asked questions about getting AWS Certified and other information about the AWS Certification: https://aws.amazon.com/certification/faqs/.</p>"},{"location":"Introduction/","title":"Introduction","text":"<p>The trend of emerging technologies that have a large impact on industries is a common theme in today\u2019s headlines. More and more companies are adopting newer technologies that will allow them to grow and increase returns. The Amazon Web Services (AWS) Cloud is one example of it. There are more than a million active users of Amazon Web Services. These users are a mix of small businesses, independent contractors, large enterprises, developers, students, and many more. There is no doubt that AWS already has a community of users, and the number of newcomers is increasing rapidly each day.</p> <p>With AWS, you don\u2019t have to purchase and manage your infrastructure. It is a considerable cost saving for your company, and it is a sigh of relief as well for your sysadmin team. Although AWS offers a huge collection of services and products that require little configuration, they also offer traditional ones like VMs and storage devices that work similarly to their physical counterparts. This means that transitioning from an on-premises type of environment to an AWS virtualized environment should be straightforward.</p> <p>We think that developers benefit the most from the cloud. You can access your applications from anywhere if you have a browser and an Internet connection (and maybe an SSH/RDP client too). You can spin up machines in a matter of minutes from a catalog of OS and versions that suit your needs. You also have control over your storage devices, which you can also provision and de-provision easily. Aside from traditional VMs, AWS also has other options for you, such as container instances, batch instances, and serverless models that can easily be integrated with APIs. You also do not need to worry about databases. You can host them on your own in VMs with your own licenses or use AWS-provided licenses, or you can even use a managed database, so you do not need to worry about infrastructure. If these are too much for a simple app that you just want to test, AWS has a platform-as-a-service solution wherein you can simply deploy your code, and the service handles the infrastructure for you. In essence, AWS has already provided the tools that its users need to quickly take advantage of the cloud.</p> <p>With all of the benefits presented in using AWS, the true value of being an AWS Certified Developer has your knowledge and skills in developing in AWS recognized by the industry. With this recognition, you\u2019ll gain more opportunities for career growth, financial growth, and personal growth as well. Certified individuals can negotiate for higher salaries, aim for promotions, or land higher job positions. Becoming an AWS Certified Developer Associate is also a great way to start on your journey in DevOps, which is one of the most in-demand and well-compensated roles in the IT industry today. This certificate is a must-have in your portfolio if you wish to progress your career in AWS.</p> <p>We took extra care to come up with these study guides and cheat sheets. However, this is meant to be  just a supplementary resource when preparing for the exam. We highly recommend working on hands-on sessions and practice exams to further expand your knowledge and improve your test-taking skills.</p>"},{"location":"Study%20Guide%20and%20Tips/","title":"Study Guide and Tips","text":"<p>The AWS Certified Developer Associate certification is for those who are interested in handling cloud-based applications and services. Typically, applications developed in AWS are sold as products in the AWS Marketplace. This allows other customers to use the customized, cloud-compatible application for their own business needs. Because of this, AWS developers should be proficient in using the AWS CLI, APIs, and SDKs for application development.</p> <p>The AWS Certified Developer Associate exam (or AWS CDA for short) will test your ability to:</p> <ul> <li>Demonstrate an understanding of core AWS services, uses, and basic AWS architecture best practices.</li> <li>Demonstrate proficiency in developing, deploying, and debugging cloud-based applications using AWS.</li> </ul> <p>Having prior experience in programming and scripting for both standard, containerized, and/or serverless applications will greatly make your review easier. Additionally, we recommend having an AWS account available for you to play around with to better visualize parts in your review that involve code. For more details regarding your exam, you can check out this AWS exam blueprint and official sample DVA-C02 questions for the AWS Certified Developer Associate exam.</p>"},{"location":"Study%20Guide%20and%20Tips/#study-materials","title":"Study Materials","text":"<p>If you are not well-versed in the fundamentals of AWS, we suggest that you visit our AWS Certified Cloud Practitioner review guide to get started. AWS also offers a free virtual course called AWS Cloud Practitioner Essentials that you can take in their training portal. Knowing the basic concepts and services of AWS will make your review more coherent and understandable for you.</p> <p>The primary study materials you\u2019ll be using for your review are the: FREE AWS Exam Readiness video course, official AWS sample questions, AWS whitepapers, FAQs, AWS cheat sheets, AWS practice exams and AWS video course with included hands-on labs.</p> <p></p> <p>For whitepapers, they include the following:</p> <ol> <li>Implementing Microservices on AWS \u2013 This paper introduces the ways you can implement a microservice system on different AWS Compute platforms. You should study how these systems are built and the reasoning behind the chosen services for that system.</li> <li>Running Containerized Microservices on AWS \u2013 This paper talks about the best practices in deploying a containerized microservice system in AWS. Focus on the example scenarios where the best practices are applied, how they are applied, and using which services to do so.</li> <li>Optimizing Enterprise Economics with Serverless Architectures \u2013 Read upon the use cases of serverless in different platforms. Understand when it is best to use serverless vs. maintaining your own servers. Also, familiarize yourself with the AWS services that are under the serverless toolkit.</li> <li>Serverless Architectures with AWS Lambda \u2013 Learn about Serverless and Lambda as much as you can. Concepts, configurations, code, and architectures are all important and are most likely to come up in the exam. Creating a Lambda function of your own will help you remember features faster.</li> <li>Practicing Continuous Integration and Continuous Delivery on AWS Accelerating Software Delivery with DevOps \u2013 If you are a developer aiming for the DevOps track, then this whitepaper is packed with practices for you to learn. CI/CD involves many stages that allow you to deploy your applications faster. Therefore, you should study the different deployment methods and understand how each of them works. Also, familiarize yourself with the implementation of CI/CD in AWS. We recommend performing a lab of this in your AWS account.</li> <li>Blue/Green Deployments on AWS \u2013 Blue/Green Deployments is a popular deployment method that you should learn as an AWS Developer. Study how blue/green deployments are implemented and know the set of AWS services used in application deployment. It is also crucial that you understand the scenarios where blue/green deployments are beneficial and where they are not. Do NOT mix up your blue environment from your green environment.</li> <li>AWS Security Best Practices \u2013 Understand the security best practices and their purpose in your environment. Some services offer more than one form of security feature, such as multiple key management schemes for encryption. It is important that you can determine which form is most suitable for the given scenarios in your exam.</li> <li>AWS Well-Architected Framework \u2013 This whitepaper is one of the most important papers that you should study for the exam. It discusses the different pillars that make up a well-architected cloud environment. Expect the scenarios in your exam to be heavily based upon these pillars. Each pillar will have a corresponding whitepaper of its own that discusses the respective pillar in more detail.</li> </ol>"},{"location":"Study%20Guide%20and%20Tips/#aws-services-to-focus-on","title":"AWS Services to Focus On","text":"<p>AWS offers extensive documentation and well-written FAQs for all of its services. These two will be your primary source of information when studying AWS. You need to be well-versed in many AWS products and services since you will almost always be using them in your work. I recommend checking out Tutorials Dojo\u2019s AWS Cheat Sheets, which provide a summarized but highly informative set of notes and tips for your review on these services.</p> <p>Services to study for:</p> <ol> <li>Amazon EC2 / ELB / Auto Scaling \u2013 Be comfortable with integrating EC2 to ELBs and Auto Scaling. Study the commonly used AWS CLI commands, APIs and SDK code under these services. Focus as well on security, maintaining high availability, and enabling network connectivity from your ELB to your EC2 instances. </li> <li>AWS Elastic Beanstalk \u2013 Know when Elastic Beanstalk is more appropriate to use than other computing or infrastructure-as-a-code solutions like CloudFormation. Experiment with the service yourself in your AWS account, and understand how you can deploy and maintain your own application in Beanstalk.</li> <li>Amazon ECS \u2013 Study how you can manage your own cluster using ECS. Also, figure out how ECS can be integrated into a CI/CD pipeline. Be sure to read the FAQs thoroughly since the exam includes multiple questions about containers.</li> <li>AWS Lambda \u2013 The best way to learn Lambda is to create a function yourself. Also, remember that Lambda allows custom runtimes that a customer can provide himself. Figure out what services can be integrated with Lambda and how Lambda functions can capture and manipulate incoming events. Lastly, study the Serverless Application Model (SAM).</li> <li>Amazon RDS / Amazon Aurora \u2013 Understand how RDS integrates with your application through EC2, ECS, Elastic Beanstalk, and more. Compare RDS to DynamoDB and Elasticache and determine when RDS is best used. Also, know when it is better to use Amazon Aurora than Amazon RDS and when RDS is more useful than hosting your own database inside an EC2 instance.</li> <li>Amazon DynamoDB \u2013 You should have a complete understanding of the DynamoDB service as this is very crucial in your exam. Read the DynamoDB documentation since it is more detailed and informative than the FAQ. As a developer, you should also know how to provision your own DynamoDB table, and you should be capable of tweaking its settings to meet application requirements.</li> <li>Amazon Elasticache \u2013 Elasticache is a caching service you\u2019ll often encounter in the exam. Compare and contrast Redis from Memcached. Determine when Elasticache is more suitable      than DynamoDB or RDS. </li> <li>Amazon S3 \u2013 S3 is usually your go-to storage for objects. Study how you can secure your objects through KMS encryption, ACLs, and bucket policies. Know how S3 stores your objects to keep them highly durable and available. Also, learn about lifecycle policies. Compare S3 to EBS and EFS to know when S3 is more preferred than the other two.</li> <li>Amazon EFS \u2013 EFS is used to set up file systems for multiple EC2 instances. Compare and contrast S3 to EFS and EBS. Study file encryption and EFS performance optimization as well.</li> <li>Amazon Kinesis \u2013 There are usually tricky questions on Kinesis, so you should read its documentation too. Focus on Kinesis Data Streams and explore the other Kinesis services. Familiarize yourself with Kinesis APIs, Kinesis Sharding, and integration with storage services such as S3 or compute services like AWS Lambda.</li> <li>Amazon API Gateway \u2013 API gateway is usually used together with AWS Lambda as part of the serverless application model. Understand API Gateway\u2019s structure, such as resources, stages, and methods. Learn how you can combine API Gateway with other AWS services, such as Lambda or CloudFront. Determine how you can secure your APIs so that only a select number of people can execute them.</li> <li>Amazon Cognito \u2013 Cognito is used for mobile and web authentication. You usually encounter Cognito questions in the exam along with Lambda, API Gateway, and DynamoDB. This usually involves some mobile application requiring an easy sign-up/sign-in feature from AWS. It is highly suggested that you try using Cognito to better understand its features.</li> <li>Amazon SQS \u2013 Study the purpose of different SQS queues, timeouts, and how your messages are handled inside queues. Messages in an SQS queue are not deleted when polled, so be sure to read on that as well. There are different polling mechanisms in SQS, so you should compare and contrast each.</li> <li>Amazon CloudWatch \u2013 CloudWatch is a primary monitoring tool for all your AWS services. Verify that you know what metrics can be found under CloudWatch monitoring and what metrics require a CloudWatch agent installed. Also, study CloudWatch Logs, CloudWatch Alarms, and Billing monitoring. Differentiate the kinds of logs stored in CloudWatch vs. logs stored in CloudTrail.</li> <li>AWS IAM \u2013 IAM is the security center of your cloud. Therefore, you should familiarize yourself with the different IAM features. Study how IAM policies are written and what each section in the policy means. Understand the usage of IAM user roles and service roles. You should have read up on the best practices whitepaper in securing your AWS account through IAM.</li> <li>AWS KMS \u2013 KMS contains keys that you use to encrypt EBS, S3, and other services. Know what these services are. Learn the different types of KMS keys and in which situations each type of key is used.</li> <li>AWS CodeBuild / AWS CodeCommit / AWS CodeDeploy / AWS CodePipeline \u2013 These are your tools for implementing CI/CD in AWS. Study how you can build applications in CodeBuild (<code>buildspec</code>), and how you\u2019ll prepare configuration files (<code>appspec</code>) for CodeDeploy. CodeCommit is a git repository, so having knowledge in Git will be beneficial. I suggest that you build a simple pipeline of your own in CodePipeline to see how you should manage your code deployments. It is also important to learn how you can roll back to your previous application version after a failed deployment. The whitepapers above should have explained in-place deployments and blue/green deployments and how to perform automation</li> <li>AWS CloudFormation \u2013 Study the structure of CloudFormation scripts and how you can use them to build your infrastructure. Be comfortable with both JSON and YAML formats. Read a bit about StackSets. List down the services that use CloudFormation in the backend for     provisioning AWS resources, such as AWS SAM, and processes, such as in CI/CD.</li> </ol> <p>Aside from the concepts and services, you should study the AWS CLI, the different commonly used APIs (for services such as EC2, EBS, or Lambda), and the AWS SDKs. Read up on the AWS Serverless Application Model (AWS SAM) and AWS Application Migration Service, as well as these that may come up in the exam. It will also be very helpful to have experience interacting with AWS APIs and SDKs and troubleshooting any errors that you encounter while using them.</p>"},{"location":"Study%20Guide%20and%20Tips/#common-exam-scenarios","title":"Common Exam Scenarios","text":""},{"location":"Study%20Guide%20and%20Tips/#lambda","title":"Lambda","text":"Scenario Solution An application running in a local server is converted to a Lambda function. When the function was tested, an Unable to import module error showed Install the missing modules in your application\u2019s folder and package them into a ZIP \ufb01le before uploading to  AWS Lambda. A Developer is writing a Lambda function that will be used to send a request to an API in different environments (Prod, Dev, Test). The function needs to automatically invoke the correct API call based on the environment. Use Environment Variables A Lambda function needs temporary storage to store \ufb01les while executing. Store the \ufb01les in the <code>/tmp</code> directory Lambda function is writing data into an RDS database. The function needs to reuse the database connection to reduce execution time. Use execution context by placing the database connection logic outside of the event handler. A Developer needs to increase the CPU available to a Lambda function to process data more e\ufb03ciently. Increase the allocated memory of the function."},{"location":"Study%20Guide%20and%20Tips/#amazon-api-gateway","title":"Amazon API Gateway","text":"Scenario Solution A Developer has an application that uses a RESTful API  hosted in API Gateway. The API requests are failing with a \"No 'Access-Control-Allow-Origin' header is present on the requested resource\" error message. Enable CORS in the API Gateway Console. A website integrated with API Gateway requires user requests to reach the backend server without intervention from the API Gateway. Which integration type should be used? <code>HTTP_PROXY</code> A serverless application is composed of AWS Lambda, DynamoDB, and API Gateway. Users are complaining about getting HTTP 504 errors. The API requests are reaching the maximum integration timeout for API Gateway (29 seconds). How to invalidate API Gateway cache? 1. Send a request with a <code>Cache-Control: max-age</code> header. 2.  Enable the <code>Require Authorization</code> option on your API cache settings. A developer needs to deploy different API versions in API Gateway Use stage variables"},{"location":"Study%20Guide%20and%20Tips/#amazon-dynamodb","title":"Amazon DynamoDB","text":"Scenario Solution A Developer needs a cost-effective solution to delete session data in a DynamoDB table. Expire session data with DynamoDB TTL New changes to a DynamoDB table should be recorded in another DynamoDB table. Use DynamoDB Streams Reduce the DynamoDB database response time. Use DynamoDB Accelerator (DAX) Choosing the best partition key for the DynamDB table. Use the partition key with the highest cardinality (e.g. student ID, employee ID) An application uses a DynamoDB database with Global Secondary Index. DynamoDB requests are returning a <code>ProvisionedThroughputExceededException</code> error. Why is this happening? The write capacity of the GSI is less than the base table."},{"location":"Study%20Guide%20and%20Tips/#cloudformation-and-aws-sam","title":"CloudFormation and AWS SAM","text":"Scenario Solution What section must be added to a CloudFormation template to include resources defined by AWS SAM? Transform A developer needs a reliable framework for building serverless applications in AWS AWS SAM A CloudFormation stack creation process failed unexpectedly. CloudFormation will roll back by deleting resources that it has already created. A CloudFormation template will be used across multiple AWS accounts Use CloudFormation StackSets"},{"location":"Study%20Guide%20and%20Tips/#deployment-and-security","title":"Deployment and Security","text":"Scenario Solution It is required that incoming traffic is shifted in two increments. 10% of the traffic must be shifted in the first increment, and the remaining 90% should be deployed after some minutes. Transform You need to authenticate users of a website using social media identity profiles. Amazon Cognito Identity Pools A company has two accounts. The developers from Account A need to access resources on Account B Use cross-account access role Multiple developers need to make incremental code updates to a single project and then deploy the new changes. Use AWS CodeCommit as the code repository and directly deploy the new package using AWS CodeDeploy. A development team is using CodePipeline to automate their deployment process. The code changes must be reviewed by a person before releasing to production Add a manual approval action stage"},{"location":"Study%20Guide%20and%20Tips/#relevant-apicli-commands","title":"Relevant API/CLI commands","text":"Scenario Solution A Developer needs to decode an encoded authorization failure message. Use the <code>aws sts decode-authorization-message</code> command. How can a Developer verify permission to call a CLI command without actually making a request? Use the <code>--dry-run</code> parameter along with the CLI command. A Developer needs to deploy a CloudFormation template from a local computer. Use the <code>aws cloudformation package</code> and <code>aws cloudformation deploy</code> command A Developer has to ensure that no applications can fetch a message from an SQS queue that\u2019s being processed or has already been processed. Increase the <code>VisibilityTimeout</code> value using the <code>ChangeMessageVisibility</code> API and delete the message using the <code>DeleteMessage</code> API. A Developer has created an IAM Role for an application that uploads files to an S3 bucket. Which API call should the Developer use to allow the application to make upload requests? Use the <code>AssumeRole</code> API"},{"location":"AWS%20Deep%20Dives/AWS%20Identity%20Access%20Management%20%28IAM%29/","title":"AWS Identity Access Management (IAM)","text":"<p>IAM is the primary tool for controlling and managing access to an AWS account. It sits at the core of AWS security; everything you do with AWS, whether it's creating a Lambda function, uploading a file to an S3 bucket, or something mundane as viewing EC2 instances on the Console, is governed by IAM. It allows you to specify who, which AWS resources, as well as what actions they can and cannot do. These are also known as authentication and authorization.</p>"},{"location":"AWS%20Deep%20Dives/AWS%20Identity%20Access%20Management%20%28IAM%29/#iam-identities","title":"IAM Identities","text":"<p>IAM identities handle the authentication aspect of your AWS account. It pertains to any user, application, or group that belongs to your organization.</p> <p></p> <p>An IAM identity can be an IAM User , IAM role , or IAM Group .</p> <p>An IAM User represents the user or application who interacts with AWS services. Take note that an IAM user is different from the root user. Although full admin permissions can be given to an IAM user, there are certain actions that only a root user can carry out, such as deleting an AWS account. IAM users are given long-term credentials for accessing AWS services. These credentials can be in the form of a username and password (for console access) or an access key and secret key (for programmatic access). The former is primarily used when logging into the AWS Console, whereas the latter is used when interacting with AWS Services via CLI/API commands.</p> <p>An IAM role isn\u2019t intended to be associated with a unique user, rather, it is meant to be assumed by certain AWS Services or a group of external users with common business roles. Unlike IAM users, roles use time-limited security credentials for accessing AWS. When creating a role, you choose a trusted entity. The trusted entity determines who is authorized to assume the IAM role.</p> <p>An IAM role can be assumed by AWS Services to perform actions on your behalf, an IAM user within your account or from an external one, and users federated by identity providers that AWS trusts. Examples of these identity providers are Microsoft Active Directory, Facebook, Google, Amazon, or any IdP that is compatible with OpenID Connect (OIDC) and SAML 2.0.</p> <p></p> <p>IAM Group is simply a collection of IAM users. The policies attached to an IAM Group are inherited by the IAM users under it. It\u2019s possible to assign an IAM user to multiple groups, but groups cannot be nested (a group within a group). IAM group offers an easy way of managing users in your account. For example, if you have a team of developers that needs access to AWS Lambda, instead of attaching the necessary permission for them individually, you can put the developers together in an IAM Group called \u2018Developers\u2019 and associate the Lambda permissions to that group. If a new member joins the team, simply add him/her to the \u2018Developers\u2019 IAM group.</p>"},{"location":"AWS%20Deep%20Dives/AWS%20Identity%20Access%20Management%20%28IAM%29/#iam-policy","title":"IAM Policy","text":"<p>An IAM identity cannot perform AWS actions without an IAM Policy attached to it unless the resource being accessed allows the IAM Identity to do so. An IAM Policy is what authorizes an IAM user or role to control and access your AWS resources. There are three types of IAM Policies to choose from:</p> <ul> <li>AWS Managed Policies - these are built-in policies that have been constructed to conform to common use cases and job roles. For example, let\u2019s say you\u2019re working as a system administrator, and you have to give your new database administrator the necessary permissions to do his/her job. Rather than figuring out the right permissions, you may simply use the DatabaseAdministrator managed policy, which grants complete access to AWS services necessary to set up and configure AWS database services. AWS Managed Policies cannot be deleted or modified. </li> <li>Customer-managed Policy - this refers to the policies that you manually create in your account.</li> <li>Inline Policy - a policy that is embedded in an IAM Identity. Unlike AWS Managed Policies and Customer-managed Policies, an inline policy does not have its own ARN; thus, it can\u2019t be referenced by other IAM identities. It is scoped to a specific IAM user or role. </li> </ul> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html</li> <li>https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</li> <li>https://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups.html</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Identity%20Access%20Management%20%28IAM%29/#iam-policy-structure","title":"IAM Policy Structure","text":"<p>An IAM Policy is expressed as a JSON document. It is made up of two components: policy-wide information and one or more statements. The policy-wide information is an optional element that is placed on top of the document. It\u2019s usually just a Version element pertaining to the AWS policy language version being used. This element usually has a static value of 2012-10-17 , referring to the release date of the current AWS policy access language.</p> <p>The Statement block is where you add the permissions you need for accessing various AWS services. A policy can have single or multiple statements where each statement is enclosed within a bracket.</p> <p>The following is an example of an IAM Policy.</p> <pre><code>{\n    \"Version\": \"2012-10-17\" ,\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowFullEC2AccessFromMyNetwork\" ,\n            \"Effect\": \"Allow\" ,\n            \"Action\": [\n                \"ec2:DescribeInstance*\" \n            ],\n            \"Resource\": \"*\" ,\n            \"Condition\": {\n                \"IpAddress\": {\n                    \"aws:SourceIp\": \"180.0.111.0/24\" \n                }\n            }\n       }\n    ]\n}\n</code></pre> <p>An individual statement has the following elements:</p> <ul> <li><code>Statement ID (Sid)</code> - this is just a label that you give to a statement. It\u2019s helpful for quickly identifying  what a statement is for rather than reviewing what\u2019s in the Action , Effect , or Resource . This element is optional; however, it might be useful when modifying or debugging policies with multiple statements.</li> <li><code>Effect</code> - explicitly dictates whether the policy allows or denies access to a given resource. It only accepts an Allow or a Deny .</li> <li><code>Action</code> - this element contains the list of actions that the policy allows or denies. You can use the () wildcard to grant access to all or a subset of AWS operations. In the example above, the ec2:DescribeInstance refers to all actions that start with the string \u201cDescribeInstance\u201d. So this can be referring to DescribeInstanceAttribute , DescribeInstances , DescribeInstancesStatus , and so on.</li> <li><code>Resource</code> - the list of AWS resources to which the Action element is applied. The example policy above uses a (*) wildcard alone to match all characters. This implies that Action is applicable to all resources. You can be more restrictive to the resources that you want to work with by specifying their Amazon Resource Names (ARN).</li> <li><code>Condition</code> - an optional element that you can use to apply logic to your policy when it\u2019s in effect. For instance, the sample policy above only allows requests originating from the 180.0.111.0/24 network. Some conditions that you should be aware of are:<ul> <li><code>StringEquals</code> - Exact string matching and case-sensitive</li> <li><code>StringNotEquals</code> - Negated matching</li> <li><code>StringLike</code> - Exact matching but ignoring case</li> <li><code>StringNotLike</code> - Negated matching</li> <li><code>Bool</code> - Lets you construct Condition elements that restrict access based on true or false values.</li> <li><code>IpAddress</code> - Matching specified IP address or range.</li> <li><code>NotIpAddress</code> - All IP addresses except the specified IP address or range</li> <li><code>ArnEquals</code>, <code>ArnLike</code></li> <li><code>ArnNotEquals</code>, <code>ArnNotLike</code></li> <li>Use a <code>Null</code> condition operator to check if a condition key is present at the time of authorization.</li> <li>You can add IfExists to the end of any condition operator name (except the <code>Null</code> condition)\u2014for example, <code>StringLikeIfExists</code>.</li> </ul> </li> </ul> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-policy-structure.html</li> <li>https://aws.amazon.com/blogs/security/back-to-school-understanding-the-iam-policy-grammar/</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Identity%20Access%20Management%20%28IAM%29/#identity-based-policy-vs-resource-based-policy","title":"Identity-based Policy vs. Resource-based Policy","text":"<p>There are six types of IAM Policy: Identity-based policies, Resource-based policies, IAM permissions boundaries, Service Control Policies, Session policies, and Access Control Lists (ACLs). For the purpose of the exam, we will focus only on Identity-based and Resource-based policies. The difference between them is pretty simple. The Identity-based policies are policies you attach to an IAM Identity, such as IAM user or IAM role. On the other hand, resource-based policies are attached to AWS resources, such as an S3 bucket (bucket policy), KMS key (key policy), or a Lambda function. Take note that not all AWS resources support resource-based policies.</p> <p></p> <p>An Identity-based policy defines the resources and actions that an IAM user or role has access to. Since the policy is attached to the IAM user or role, the Principal element doesn't need to be explicitly specified. In other words, the IAM user or role that the policy is attached to is implicitly considered the principal. </p> <p>A Resource-based policy , on the other hand, must include both the Principal and Resource elements. The Principal element specifies which IAM identities are allowed to access the resource, while the Resource element specifies which resources users are allowed to perform actions on. For example, a bucket policy is a type of resource-based policy that is attached to an Amazon S3 bucket. The policy specifies the actions that are allowed on the bucket and the IAM users or roles that are allowed to perform those actions.</p> <p>The following S3 bucket policy is an example of how the Resource element is used to grant resource-level permissions. You can see that the tdojo S3 bucket contains two folders, john-folder and dave-folder, to which IAM users John and Dave have read access, respectively.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\"AWS\": \"arn:aws:iam::123456789000:user/john\"},\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::tdojo/john-folder\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\"AWS\": \"arn:aws:iam::123456789000:user/dave\"},\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::tdojo/dave-folder\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>An important aspect of IAM that you need to understand is how different policies are evaluated. Before determining whether your request is allowed or not, AWS evaluates all explicit DENY statements first on all policies that are involved. If the requester is specified in a DENY statement in either of the policies, IAM denies the request. Second, between the resource-based and identity-based policies, resource-based policies are evaluated first. For example, an IAM user with an empty IAM policy will still be able to access an S3 bucket as long as its bucket policy allows the IAM user to perform actions on the bucket.</p> <p>Here's a stripped-down version of the AWS flow chart on policy evaluation. Only resource-based and IAM-based policies are included for simplicity.</p> <p></p> <p>References:</p> <ul> <li> <p>https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html</p> </li> <li> <p>https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html</p> </li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Identity%20Access%20Management%20%28IAM%29/#cross-account-access","title":"Cross-account access","text":"<p>The policy evaluation logic discussed in the previous section is for Intra-account access which involves users and AWS resources interacting with each other within the same account. The evaluation of policies is quite different when it comes to cross-account access. </p> <p>Cross-account access refers to access between two or more separate AWS accounts. This could be an EC2 instance in one AWS account accessing an S3 bucket in a different AWS account or a user in one account assuming a role in another account to access resources in that account.</p>"},{"location":"AWS%20Deep%20Dives/AWS%20Identity%20Access%20Management%20%28IAM%29/#cross-account-for-iam-users","title":"Cross-account for IAM users","text":"<p>Imagine an IAM User named \u2018Tucker\u2019 in Account A who needs to download documents from the <code>private/documents</code> folder of an S3 bucket in Account B.</p> <p>To grant this permission, the following actions must be configured:</p> <ol> <li> <p>In Account B, the S3 bucket owner creates an S3 bucket policy that specifies Tucker is allowed to download the documents. The policy should be attached to the <code>private</code>  S3 bucket in Account B.</p> <p>Example S3 bucket policy in Account B:</p> </li> </ol> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\"AWS\": \"arn:aws:iam::&lt;Account A ID&gt;:user/Tucker\"},\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::private/documents/*\"\n        }\n    ]\n}\n</code></pre> <ol> <li> <p>In Account A, Tucker must have an IAM policy that grants permissions to access the S3 bucket in Account B. The policy allows Tucker to access the S3 bucket without assuming an IAM role.</p> <p>Example IAM policy for Tucker in Account A: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"DownloadDocuments\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\"Action\": \"s3:GetObject\" ],\n            \"Resource\": \"arn:aws:s3:::private/documents/*\"\n        }\n    ]\n}\n</code></pre></p> </li> </ol> <p>After setting up the required permissions in both accounts, Tucker can now use AWS SDK or CLI to access the S3 bucket in Account B and download the documents.</p>"},{"location":"AWS%20Deep%20Dives/AWS%20Identity%20Access%20Management%20%28IAM%29/#how-is-access-evaluated","title":"How is access evaluated?","text":"<p>If Tucker makes a request to the bucket in Account B, AWS performs two evaluations to determine whether the request is allowed or denied, one in the trusting account (AccountB) and one in the trusted account (AccountA). In Account A, AWS checks the identity-based policy of Tucker and any policies that can limit actions that he\u2019s requesting. In AccountB, AWS evaluates the bucket policy and any policies that can limit the action being requested. The request is only allowed if both evaluations result in a decision of \"Allow\". If any policy evaluation returns a decision of \"Deny\", the access request is denied.</p>"},{"location":"AWS%20Deep%20Dives/AWS%20Identity%20Access%20Management%20%28IAM%29/#cross-account-for-iam-roles","title":"Cross-account for IAM roles","text":"<p>Alternatively, an administrator in Account B can create an IAM role for Tucker to assume. This method provides a more scalable and flexible solution, as the administrator can easily modify or revoke access to the S3 bucket as needed. For instance, if there are other IAM users in Account A that need similar access to the bucket, the administrator can simply grant permission for them to assume the IAM role rather than modifying the S3 bucket policy for each user. This approach reduces administrative overhead in granting access to the bucket. </p> <p>Here are the steps for how you might grant any IAM users in Account A to assume an IAM role in Account B.</p> <ol> <li> <p>In Account B, the IAM administrator creates an IAM role with a policy that grants access to the S3 bucket.</p> <p>Example S3 bucket policy in Account B:</p> </li> </ol> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"DownloadDocuments\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:GetObject\"],\n      \"Resource\": \"arn:aws:s3:::private/documents/*\"\n    }\n  ]\n}\n</code></pre> <ol> <li> <p>The IAM administrator must also attach a trust policy to the IAM role, specifying which AWS accounts and entities are allowed to assume the role. In this case, the trust policy must include Account A as a trusted account.</p> <p>Example trust policy in Account B (Note that the \u201croot\u201d refers to any Principals in Account A, excluding the root user):</p> </li> </ol> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\"AWS\": \"arn:aws:iam::AccountA:root\"},\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n</code></pre> <ol> <li> <p>The policy for an IAM user in Account A must allow the sts:AssumeRole action on the ARN of the IAM role in Account B.</p> <p>Example IAM policy for an IAM user in Account A:</p> </li> </ol> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"sts:AssumeRole\",\n      \"Resource\": \"arn:aws:iam::&lt;Account B ID&gt;:role/RoleName\"\n    }\n  ]\n}\n</code></pre>"},{"location":"AWS%20Deep%20Dives/AWS%20Identity%20Access%20Management%20%28IAM%29/#how-is-access-evaluated_1","title":"How is access evaluated?","text":"<p>Once the trust policy is evaluated in Account B, AWS verifies whether the IAM user (in Account A) assuming the IAM role is authorized to do so. Once the IAM user assumes the role, AWS checks if the IAM role has sufficient permissions to access the requested resource in the bucket. After that, the S3 bucket policy is evaluated and access will be granted only if all policy evaluations result in a decision of \"Allow\". </p> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic-cross-account.html</li> <li>https://aws.amazon.com/blogs/security/how-to-use-trust-policies-with-iam-roles/</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Identity%20Access%20Management%20%28IAM%29/#iampassrole-permission","title":"IAM:PassRole Permission","text":"<p>PassRole is a special type of IAM permission that permits a user to associate IAM roles to an AWS resource. This is a simple yet powerful permission that warrants due scrutiny when constructing IAM Policies. You can, for example, use the PassRole permission to prohibit certain IAM users from passing roles that have greater permissions than the user is allowed to have. Let me paint a scenario for you to explain this.</p> <p>Say your company has a Lambda function that processes data stored on Amazon S3. An execution role with full S3 access is attached to the Lambda function. The Lambda function runs per schedule and only admins are authorized to make code changes to it. Even though you don\u2019t have access to the Lambda function, you can bypass the permissions given to you if the following policy is attached to your IAM user.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"iam:PassRole\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre> <p>The preceding IAM policy allows you to pass any IAM roles that exist on your company\u2019s AWS account. This creates a security hole that can be exploited to elevate your access. Given you have enough permissions to create Lambda functions, all you have to do is launch a fresh Lambda function and attach the execution role that has full S3 access. Doing so, even if your IAM user lacks S3 permissions, you\u2019d still be able to access Amazon S3 using the Lambda function.</p> <p></p> <p>For better security, be more granular when it comes to providing access. For example, list the specific IAM roles that a user can pass rather than just using a wildcard, like what\u2019s shown below:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"iam:PassRole\",\n      \"Resource\": [\n        \"arn:aws:iam::123456789123:role/td-cloudwatch-agent-role\",\n        \"arn:aws:iam::123456789123:role/td-s3-role\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html</li> <li>https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_iam-passrole-service.html</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Identity%20Access%20Management%20%28IAM%29/#additional-notes","title":"Additional Notes","text":"<p>In AWS, Role-Based Access Control (RBAC) is implemented using AWS Identity and Access Management (IAM). RBAC in AWS allows you to define permissions based on roles that align with specific job functions within your organization. Each role has an associated set of permissions specified by IAM policies. These roles can be allocated to anyone, including groups or services, granting them the access they need to carry out their responsibilities. As an example, you can define roles like Admin, Developer, or ReadOnly User, each of which has certain rights that limit what they can do and on which AWS resources. This method simplifies the management of access controls by grouping permissions based on roles rather than managing permissions for each individual user . </p> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/prescriptive-guidance/latest/saas-multitenant-api-access-authorization/access-control-types.html</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/","title":"AWS Lambda","text":"<p>AWS Lambda lets you run codes without managing servers. You don't need to worry about tasks such as scaling, patching, and other management operations that are typically done on EC2 instances or on-premises servers. You can allot the maximum memory available for a Lambda function, as well as the function's execution duration, before timing out. The memory, which scales proportionally to the CPU power, can range from 128 MB to 10,240 MB in 1-MB increments. The default timeout is three seconds, with a maximum value of 900 seconds (15 minutes).</p> <p>A Lambda function can be invoked in different ways. You can invoke a function directly on the AWS Lambda console, via the <code>Invoke</code> API/CLI command, or through a Function URL . You can set up certain AWS Services to invoke a Lambda function as well. For example, you can create a Lambda function that responds to Amazon S3 events (e.g. processing files as they are uploaded to an S3 bucket) or set up an Amazon EventBridge rule that triggers a Lambda function every week to perform batch processing. Lambda functions are also commonly used as a backend for APIs that do not require constant load, such as handling login requests or on-the-fly image processing.</p> <p></p>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/#synchronous-vs-asynchronous-invocations","title":"Synchronous vs. Asynchronous Invocations","text":"<p>There are two invocation types in AWS Lambda.</p>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/#synchronous","title":"Synchronous","text":"<p>The first type is called Synchronous invocation, which is the default. Synchronous invocation is pretty straightforward. When a function is invoked synchronously, AWS Lambda waits until the function is done processing, then returns the result. Let\u2019s see how this works through the following example:</p> <p></p> <p>The image above illustrates a Lambda function-backed API that is managed by API Gateway. When API Gateway receives a GET request from the /getOrder resource, it invokes the getOrder function. The function receives an event containing the payload, processes it, and then returns the result. </p> <p>Considerations when using synchronous invocation:</p> <ul> <li>If you\u2019re planning to integrate AWS Lambda with API Gateway, take note that API Gateway\u2019s integration timeout is 29 seconds. If a function takes longer than that to complete, the connection will time out, and the request will fail. Hence, use synchronous invocations for applications that don\u2019t take too long to complete (e.g., authorizing requests, and interacting with databases). </li> <li>Synchronously-invoked functions can accept a payload of up to 6 MB.</li> <li>You might need to implement a retry logic in your code to handle intermittent errors.</li> </ul> <p>To call a Lambda function synchronously via API/CLI, set RequestResponse as the value for the <code>invocation-type</code> parameter when calling the Invoke command, as shown below:</p> <pre><code>aws lambda invoke \\\n    --function-name testFunction \\\n    --invocation-type RequestResponse \\\n    --cli-binary-format raw-in-base64-out \\\n    --payload '{ \"input\" : \"input_value\" }' response.json\n</code></pre> <p>Alternatively, you may just omit the <code>invocation-type</code> parameter as AWS Lambda invokes functions synchronously by default.</p> <p>Services that invoke Lambda functions synchronously (services irrelevant to the exam are excluded): * Amazon API Gateway * Application Load Balancer * Amazon Cognito * Amazon Data Firehose * Amazon CloudFront (Lambda@Edge)</p>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/#asynchronous","title":"Asynchronous","text":"<p>An Asynchronous invocation is typically used when a client does not need to wait for immediate results from a function. Some examples of these are long-latency processes that run in the background, such as batch operations, video encoding, and order processing.</p> <p>When a function is invoked asynchronously, AWS Lambda stores the event in an internal queue that it manages. Let\u2019s understand asynchronous invocation through the example below:</p> <p></p> <p>A PUT request is made to the <code>/putOrder</code> resource. Like the previous example, the request goes through API Gateway, which produces an event. This time, instead of API Gateway directly invoking the function, AWS Lambda queues the event. If the event is successfully queued, AWS Lambda returns an empty payload with <code>HTTP 202 status code</code>. The <code>202 status code</code> is just a confirmation that the event is queued; it\u2019s not indicative of a successful invocation. The client will not be required to wait for the Lambda function to complete. So, to improve user experience, you can create a worker that tracks order completion and sends notifications once the order is successfully processed.</p> <p>To call a Lambda function asynchronously via the Invoke command, simply set Event as the value for the invocation-type parameter, as shown below:</p> <pre><code>aws lambda invoke \\\n    --function-name testFunction \\\n    --invocation-type Event \\\n    --cli-binary-format raw-in-base64-out \\\n    --payload '{ \"input\" : \"input_value\" }' response.json\n</code></pre> <p>Considerations when using asynchronous invocation:</p> <ul> <li>Asynchronously-invoked functions can only accept a payload of up to 256 KB.</li> <li>The Lambda service implements a retry logic for asynchronously-invoked functions</li> <li>Good for applications that run in the background (batch processing, video encoding)</li> </ul> <p>Services that invoke Lambda functions asynchronously (services irrelevant to the exam are excluded):</p> <ul> <li>Amazon API Gateway (by specifying Event in the <code>X-Amz-Invocation-Type</code> request header of a non-proxy integration API)</li> <li>Amazon S3</li> <li>Amazon CloudWatch Logs</li> <li>Amazon EventBridge</li> <li>AWS CodeCommit</li> <li>AWS CloudFormation</li> <li>AWS Config</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/#handling-failed-asynchronous-invocations","title":"Handling failed asynchronous invocations","text":"<p>AWS Lambda has a built-in retry mechanism for asynchronous invocations. If the function returns an error, Lambda will attempt to retry the request two more times, with a longer wait interval between each attempt.</p> <p></p> <p>The request is discarded after AWS Lambda has exhausted all remaining retries. To avoid losing events, you may redirect failed request attempts to an SQS dead-letter queue so that you can debug the error later and have another function retry it.</p> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/lambda/latest/dg/lambda-invocation.html</li> <li>https://aws.amazon.com/blogs/architecture/understanding-the-different-ways-to-invoke-lambda-functions/</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/#event-source-mappings","title":"Event source mappings","text":"<p>Stream or queue-based resources such as DynamoDB streams, SQS queues, and Kinesis Data Streams streams do not invoke Lambda functions directly. Typically, we read records from these resources using pollers. A poller is an application that periodically checks a queue, pulls records from it (sometimes in batches), and sends them to a downstream service that will process them.</p> <p></p> <p>An event source mapping is a sort of polling agent that Lambda manages. Event source mappings take away the overhead of writing pollers from scratch to retrieve messages from queues/streams. This allows you to focus on building the domain logic of your application.</p> <p>Event source mapping invokes a function synchronously if one of the following conditions is met:</p> <ol> <li>The batch size is reached - The minimum batch size can be set to 1, but the default and maximum batch sizes vary on the AWS service that invokes your function.</li> <li>The maximum batching window is reached - The batching window is the amount of time Lambda waits to gather and batch records. The default batch window for Amazon Kinesis, Amazon DynamoDB, and Amazon SQS is 0. This means that a Lambda function will receive batches as quickly as possible. You can tweak the value of the batch window based on the nature of your application.</li> <li>The total payload is 6 MB - Because event source mappings invoke functions synchronously, the total payload (event data) that a function can receive is also limited to 6MB (the limit for synchronous invocations). This means that if the maximum record size in a queue is 100KB, then the maximum batch size you can set is 60.</li> </ol>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/#event-filtering","title":"Event filtering","text":"<p>A Lambda function is billed based on how long it runs. How often functions are invoked plays a major factor as well. This is why Lambda is great for scheduled jobs, short-duration tasks, and event-based processes. But does this mean you shouldn\u2019t use them for high-volume traffic applications? There is no definitive answer as it will always be on a case-to-case basis. It usually depends on factors such as the requirements of your application and the cost trade-off you\u2019re willing to make. Regardless, if you ever find yourself wanting to use Lambda in a high-activity application like stream processing, it\u2019s good to know that there are methods available to offset the cost of running functions.</p> <p>Batching is one cost optimization technique for Lambda functions. By increasing the batch size value, you can reduce the frequency at which your function runs. In addition, you can also filter events that are only needed by your function, thus lowering the cost even further.</p> <p>Consider the following scenario:</p> <p>You\u2019re tasked to develop a function that reacts to voltage drops or temperature changes that might indicate faulty components in a site. Multiple sensors send readings to a Kinesis Data Stream stream, which must be consumed and processed by a Lambda function.</p> <p>Your first instinct might be to implement filtering logic within the Lambda function, as shown below:</p> <pre><code>def lambda_handler (event, context):\n    temperature = event[\"temperature\"]\n    voltage = event[\"voltage\"]\n    if temperature &gt; 100 or voltage &lt; 5 :\n        #do something\n    else :\n        #do nothing\n</code></pre> <p>The code may be valid, but it is deemed inefficient due to cost as the Lambda function will be invoked unnecessarily with every sensor data transmission, even if it doesn't require processing. Supposed sensor data is sent every second. This results in 60 invocations per minute at a batch size of 1 and batching window of 0. This can lead to a waste of time and resources if none of the readings contain the desired voltage and temperature values.</p> <p></p> <p>To address this, rather than doing conditional checks for each event at the function level, you should filter the events before they are passed down to your function. Going back to the scenario, since you\u2019re only interested in specific values of voltage and temperature, you can specify a filter pattern using the <code>filter-criteria</code> parameter of the <code>CreateEventSourceMapping</code> command.</p> <pre><code>aws lambda create-event-source-mapping \\\n--function-name process-sensor-readings \\\n--batch-size 10 \\\n--starting-position LATEST \\\n--event-source-arn arn:aws:kinesis:us-east-1:123456789123:stream/sensors \\\n--filter-criteria '{\"Filters\": [{\"Pattern\": \"{\\\"temperature\\\": [{\\\"numeric\\\": [\\\"&gt;\\\", 100]}]}\"},{\\\"voltage\\\": [{\\\"numeric\\\": [\\\"&lt;\\\", 5]}]}]}'\n</code></pre> <p>A single event source can have up to five unique filters. If an event matches any of the filters, Lambda routes it to your function. Otherwise, the event is discarded. The exam won\u2019t expect you to be an expert in event filtering, but you should be familiar with it at a high level. You may also refer to this for the full list of filter syntax.</p> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/lambda/latest/dg/invocation-eventfiltering.html</li> <li>https://aws.amazon.com/blogs/compute/filtering-event-sources-for-aws-lambda-functions/</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/#execution-environment-lifecycle","title":"Execution Environment Lifecycle","text":"<p>Lambda functions undergo three phases when invoked. These are:</p> <ol> <li> <p><code>INIT Phase</code></p> <ul> <li>Occurs when a Lambda function is invoked for the first time after being deployed or after a long period of inactivity.</li> <li>Consists of two stages: environment creation and code initialization.</li> <li>Environment creation - AWS Lambda creates an instance of a function in an isolated and secure environment inside a micro virtual machine . This 'execution environment' is where the Lambda function code actually runs. Under the hood, AWS Lambda uses a virtualization technology called Firecracker to provision environments. Firecracker makes use of lightweight micro virtual machines, allowing AWS Lambda to create environments quickly, even with a high volume of requests, without sacrificing security or performance. The amount of CPU and memory allocated to the execution environment is determined by the memory settings that you've configured for your Lambda function.</li> <li>Initialization - after setting up the environment, Lambda pulls your code from Amazon S3 (Lambda function codes are securely stored in Amazon S3) and runs the initialization code. Initialization code is any code written outside of the handler function. These could be your imported dependencies, global variables, objects, etc.  </li> <li>The time it takes for the INIT phase to complete adds latency to your function's total execution  time. This \u2018delay\u2019 caused by the bootstrapping of execution environments is also known as <code>cold start</code> . AWS does not charge for the cold start that happens during the INIT phase; however, initialization times must not exceed 10 seconds</li> </ul> </li> <li> <p><code>INVOKE Phase</code> </p> <ul> <li>This is the stage at which the handler function is run. Once the handler function is done processing, AWS Lambda keeps its execution environment warm (on standby) for a period of time. This allows the function to accept subsequent invocation requests without having to create a new execution environment. As a result, the total execution time of your function is shortened because Lambda does not have to repeat everything that was done during the INIT phase. </li> </ul> </li> <li> <p><code>SHUTDOWN Phase</code></p> <ul> <li>When an execution environment stops getting invocation requests for quite some time, AWS Lambda terminates it. There is no documented time limit, but based on experience, execution environments are normally kept warm for 5-15 minutes, though this can vary.</li> </ul> </li> </ol> <p>The image below illustrates the lifecycle of a Lambda function.</p> <p></p> <p>Reference:</p> <ul> <li>https://docs.aws.amazon.com/lambda/latest/dg/runtimes-context.html</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/#reducing-cold-starts","title":"Reducing Cold Starts","text":"<p>The more libraries/packages you include, the longer your function\u2019s cold start. That said, you have to be aware and careful in choosing which dependencies to import. There are two approaches you can take to deal with cold starts. The first one is to only load what you need. This method is completely free. You simply have to be more specific and concise when it comes to the resources that you include in your function. In doing so, fewer data will be initialized.</p> <p>The second is by using Provisioned Concurrency. Provisioned Concurrency is a Lambda feature that allows you to allot pre-warmed environments for your Lambda functions. This will provide your application with constant latency, allowing it to reply to queries in the low double-digit milliseconds. Take note that Provisioned concurrency is different from Reserved concurrency. However, the concept of shared concurrency is the same. The concurrency limit tied to a Region is divided between Shared concurrency and Provisioned concurrency.</p> <p>References:</p> <ul> <li>https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/</li> <li>https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-2/</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/#execution-environment-reuse","title":"Execution Environment Reuse","text":"<p>Re-using your existing execution environment is all about reusing the config settings, dependencies, global variables, or even database connections that were already initialized during the INIT phase. To reuse, simply place all global variables, database connections, or SDK clients that you have outside of your handler function. </p> <p>Below is an example of a Lambda function code where an initialized resource is not taken advantage of. You can see that the database client is written inside the function handler.</p> <p></p> <p>If this is the case, your function will open a new database connection every time it is invoked, which is costly compute-wise and adds latency to the execution time. To eliminate this extra time, move the connection object outside the handler function so that it is only initialized once. This way, subsequent requests would be able to reconnect to the existing connection. You are charged for the time it takes your function to run, so a shorter execution duration means you\u2019d pay less.</p> <p>In addition, keep in mind that each execution environment offers a 512 MB - 10 GB of temporary storage that can be accessed at the /tmp directory. This temporary storage is quite useful for caching any data that your function needs to process at every invocation. Imagine a Lambda function that downloads an S3 file into memory each time it is called. This is inefficient if the file is rarely updated and read by multiple users. Also, because of resource constraints or code performance issues, it is not always feasible to load large data into memory at once. To solve this, instead of directly consuming data from memory, store the data in the <code>/tmp</code> directory and process it in chunks. As a result, the next time the same file is requested, your Lambda function will simply retrieve it from its local storage rather than the S3 bucket.</p> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/lambda/latest/operatorguide/execution-environment.html</li> <li>https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/#environment-variables","title":"Environment variables","text":"<p>An environment variable is a <code>key-value</code> pair that you can store and retrieve in your Lambda function. Environment variables are tied to a function and its specific version, which means no other functions can access it except for the function it is associated with. Environment variables are shared between execution environments that belong to the same function.</p> <p>A common scenario where environment variables are useful is when you want to configure different parameters for an application without touching the code. Say you have a Lambda function that needs to be tested against a development API before using the production API. In this case, instead of hardcoding the API endpoints and keys within the function code, you create two environment variables and pull them using the methods available to the programming language that you\u2019re using. That way, you\u2019d be able to change parameters tied to different environments without editing and redeploying your code. Just update the values of your environment variables. Making manual edits in the actual code isn\u2019t a big deal, but it can be cumbersome and can lead to errors, so this environment variable is a nifty feature to help you write clean and reusable serverless functions.</p> <p>There are 4 things that you must consider when creating an environment variable:</p> <ol> <li>Keys must start with a letter and are at least two characters.</li> <li>Keys must consist of only letters, numbers, and the underscore character (_).</li> <li>There are environment variables that AWS Lambda uses during the INIT phase. These are called reserved environment variables. A key for a reserved environment variable cannot be used in your function configuration. For example, if you\u2019re using an AWS SDK, you might define a variable called \u2018AWS_REGION\u2019. This will end up to an error since AWS_REGION is a reserved variable.</li> <li>The total size of all environment variables must not exceed 4 KB.</li> </ol> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html</li> <li>https://aws.amazon.com/premiumsupport/knowledge-center/lambda-common-environment-variables/</li> <li>https://aws.amazon.com/premiumsupport/knowledge-center/lambda-environment-variables-iam-access/</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/#lambda-function-url","title":"Lambda function URL","text":"<p>You can optionally configure an HTTPS endpoint (a.k.a <code>function URL</code>) that maps to a specific alias or version of your Lambda function. Like any other URLs, function URLs can be accessed via web browsers or through HTTP clients like <code>urllib3</code>, <code>request</code> (in Python), or <code>axios</code> (in Node.Js).</p> <p>When you enable function URL, you get to choose between two auth types: <code>AWS_IAM</code> and <code>NONE</code>. <code>AWS_IAM</code>, as the name implies, is only applicable to IAM identities; the requester must be an IAM user or IAM role. AWS Lambda will make a decision on whether to grant access to a function based on the IAM user's or role's permissions.</p> <p>Function URLs with a <code>NONE</code> auth type, on the other hand, let a Lambda function be invoked publicly. At first glance, it sounds risky, and you may wonder why anyone would ever do this. When you use the NONE auth type, AWS Lambda is no longer responsible for authenticating requests. This gives you the freedom to implement your own authentication logic. You may, for example, allow access to your Lambda function to only those who are logged in to your website. Aside from that, you may also configure a CORS setting to specify the domain/s from which invocation requests must originate.</p> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html</li> <li>https://aws.amazon.com/blogs/aws/announcing-aws-lambda-function-urls-built-in-https-endpoints-for-single-function-microservices/</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/#deploying-codes-with-external-dependencies","title":"Deploying Codes with External Dependencies","text":"<p>As developers, whether you\u2019re coding at work or building your own project, it is highly likely that you import at least one module, framework, or library. Libraries save time and make you more productive. You wouldn't want to reinvent the wheel and build functions from scratch unless you\u2019re facing a unique case that has not been solved before. If you\u2019re going to be building applications in AWS Lambda, it\u2019s essential that you know how to include dependencies in your Lambda functions. </p> <p>There are dependencies that come prebuilt to a specific runtime and there are also external ones. The former can be used out of the box. The latter is usually downloaded from a public repository and installed on your computer. Often, this is done with the help of a package manager (e.g., npm for NodeJs , pip for Python ). AWS Lambda does not have a terminal where you can run pip/npm commands to install external dependencies on an execution environment. And it wouldn\u2019t make sense to have one since Lambda functions are run on temporary environments.</p> <p>To deploy codes with external dependencies to AWS Lambda, do the following steps:</p> <ol> <li>Install all external dependencies locally on your application\u2019s folder.</li> <li>Create a deployment package by zipping up the project folder. You can achieve this using the native Windows zip utility or zip command in Linux.</li> <li>Upload the deployment package to AWS Lambda. You can send the file directly to the AWS Lambda Console or store it first to Amazon S3 and deploy it from there.</li> </ol> <p>The first step is always the most important. See to it that you install dependencies required by your application locally on your folder and make sure that you correctly reference them in your code. If not done correctly, you might encounter an <code>Unable to import module error</code> , which could mean two things: it\u2019s either you\u2019ve placed the dependencies on a location that Lambda couldn\u2019t find, or it is simply non-existent. </p> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/lambda/latest/dg/python-package.html</li> <li>https://aws.amazon.com/premiumsupport/knowledge-center/build-python-lambda-deployment-package/</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/#lambda-layers","title":"Lambda Layers","text":"<p>Bundling external dependencies along with the function code gets the job done, but it has some drawbacks. First, deploying multiple Lambda functions that have the same dependencies is inefficient and would just bloat the total size of your functions. Second, updating dependencies takes time and effort. Updating each function that shares similar dependencies can slow down the development process. Lambda Layers solves these problems.</p> <p></p> <p>Lambda Layers lets you store any additional code (e.g., external dependencies, custom runtimes) separately from your deployment package. Just like a deployment package, external dependencies must be in zip format before being uploaded to AWS Lambda.</p> <p>Lambda layers can be shared among Lambda functions, making it convenient and easy to update dependencies. In addition, the deployment process becomes more modularized, and your deployment package becomes significantly smaller.</p> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/serverlessrepo/latest/devguide/sharing-lambda-layers.html</li> <li>https://aws.amazon.com/blogs/aws/new-for-aws-lambda-use-any-programming-language-and-share-common-components</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/#concurrency-and-throttling","title":"Concurrency and Throttling","text":"<p>AWS Lambda is inherently scalable, but like many other things, there\u2019s a limit to how much it can scale to. It is important that you become aware of this, especially when you\u2019re building a high-throughput application using Lambda functions.</p> <p>Concurrency is the number of Lambda function executions that can run simultaneously for a period of time. By default, AWS imposes a limit of 1,000 unreserved concurrent executions across all Lambda functions for every region per account. But you can overcome this limit and further increase concurrent executions up to hundreds of thousands by contacting AWS Support.</p> <p>You can strategically distribute concurrencies across your functions, but the unreserved count should not go below 100. This means that the default maximum value of allowable reserved concurrent executions is limited to 900. The reserved concurrent executions allocated to a function set the maximum number of concurrent instances for that function.</p> <p>Example:</p> <ul> <li>Lambda function A<ul> <li>reads and uploads files to Amazon S3.</li> <li>150 concurrent executions are reserved for this function.</li> </ul> </li> <li>Lambda function B<ul> <li>writes and updates items in a DynamoDB table</li> <li>250 concurrent executions are reserved for this function.</li> </ul> </li> <li>Lambda function C<ul> <li>processes HTTP requests from API Gateway</li> <li>unreserved concurrency.</li> </ul> </li> </ul> <p>Simply adding the concurrent executions of functions A and B gives us a total reserved concurrency of 400. That leaves us with 600 unreserved concurrency executions. So what happens when the demand for Lambda function A exceeds 150 concurrent executions? This is where throttling comes into play. When a function hits its maximum concurrency limit, AWS Lambda will reject incoming invocations and return a 429 status code throttling error. It is not possible for Lambda function A to borrow from the remaining unreserved concurrency pool.</p> <p>When computing the Concurrency limits for a Lambda function, consider two things:</p> <ol> <li>Execution time</li> <li>Number of requests handled per second (requests per second)</li> </ol> <p>Example:</p> <ul> <li>Lambda function<ul> <li>processes HTTP requests from API Gateway with unreserved concurrency</li> <li>Expects an average execution time of 10 seconds</li> <li>Expects 150 requests per second.</li> </ul> </li> </ul> <p>Multiplying the average execution time by the number of requests per second gives us 1,500 concurrency executions. This is beyond the default limit. If you bring this function to production, you\u2019ll end up with a lot of failed requests due to throttling. One of the things you could try doing in this scenario is to optimize the Lambda code and hope to reduce the execution time. A sure alternative would be to increase the default limit by contacting AWS Support.</p> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html</li> <li>https://aws.amazon.com/blogs/compute/managing-aws-lambda-function-concurrency/</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/#connecting-a-lambda-function-to-a-vpc","title":"Connecting a Lambda Function to a VPC","text":"<p>If you want your Lambda function to interact with resources (e.g., RDS database, EC2 instance) inside a private subnet, you won't be able to do so by default. The reason for this is that Lambda functions live in an isolated and secured VPC managed by AWS. This is why when you create a Lambda function, you don\u2019t go through any networking configurations (VPC, subnet, ENIs), unlike when creating EC2 instances. You cannot establish a VPC peering connection between the VPC where Lambda functions are run and the VPC where your private resources are located because the former is not accessible to customers. The proper way is to configure your function to connect to a VPC.</p> <p>To connect your Lambda function to a VPC, first, make sure that your function\u2019s execution role has the required permissions to manage the creation and deletion of Elastic Network Interfaces (ENI). This is needed because AWS Lambda creates and deletes elastic network interfaces on subnets that you specify in your function\u2019s VPC configuration. AWS uses an internal service called AWS Hyperplane, which serves as a NAT service, connecting Lambda functions to the ENIs in your VPC. Thankfully, there\u2019s the <code>AWSLambdaVPCAccessExecutionRole</code> managed IAM policy, which contains the permissions needed for the job.</p> <p>Next, specify the VPC where your private resources are located under the Lambda function\u2019s network settings. When you create a VPC configuration, you get to choose the subnets where the ENIs are deployed and a security group that controls the traffic between your Lambda function and VPC. </p> <p></p> <p>Once connected, your Lambda function will lose internet access. This happens due to the fact that AWS Lambda only assigns private IP addresses to ENIs that it creates. Even if your function is connected to a public subnet, your VPC's internet gateway will still be unable to route traffic between the internet and your function. To give your Lambda function internet access, create a NAT Gateway in the public subnet of your function's VPC and add an entry to the private subnet's route table. Set 0.0.0.0/0 as the destination and the NAT Gateway as the target.</p> <p></p> <p>References:</p> <ul> <li>https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function</li> <li>https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20Lambda/#versions-and-aliases","title":"Versions and Aliases","text":"<p>Revisions are almost unavoidable when developing applications, whether you\u2019re adding new features or fixing bugs. Instead of directly making changes to the stable version of your Lambda function, you can publish new versions of it where you can play around and experiment at will without affecting the stable version. </p> <p>A version is a snapshot of a Lambda function\u2019s state at a given time. When you publish a new version, a :version-number suffix is appended to your function\u2019s ARN, which indicates its version. An example is shown below:</p> <pre><code>arn:aws:lambda:us-east-2:123456789123:function:cool-function :1\n</code></pre> <p>AWS Lambda assigns version numbers monotonically. This means that if you delete a version and publish another one, the sequencing of numbers as you add versions will not reset; rather, it\u2019ll just increment.</p> <p>The unpublished version is also referred to as the <code>$LATEST</code> version. If you call a function without any suffix, AWS Lambda will implicitly invoke the <code>$LATEST</code>version. Take note that published versions are immutable; any modifications to the code or function configuration are not possible. If you want to make code or config updates, apply the changes to the <code>$LATEST</code> version and publish it as another version.</p> <p></p> <p>The problem with versions is that you have no control over what is appended to the function's ARN. It can be bothersome to have to update the function's ARN in your code or app parameters every time a new version is released. Wouldn't it be great if there\u2019s a pointer that we can use to switch between versions? In this way, you can simply set and forget the ARN of the function. This is where Aliases come into play. You can think of an alias as a nickname that you give to a version number. </p> <p>To invoke a function with an alias, simply append a <code>:alias-name</code> suffix to your function\u2019s ARN, just like the example below:</p> <pre><code>arn :aws:lambda:us-east-2:123456789123:function:cool-function : PROD\n</code></pre> <p>You can change the version that the alias is pointing to in the Lambda Console or via the UpdateAlias API.</p> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</li> <li>https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20STS/","title":"AWS STS","text":"<p>AWS Security Token Service (AWS STS) is a global web service that allows you to generate temporary access for IAM users or federated users to gain access to your AWS resources. These temporary credentials are session-based, meaning they\u2019re for short-term use only; once expired, they can no longer be used to access your AWS resources.</p> <p>AWS STS can\u2019t be accessed on the AWS console; it is only accessible through API. All STS requests go to a single endpoint at https://sts.amazonaws.com/, and logs are then recorded to AWS CloudTrail.</p>"},{"location":"AWS%20Deep%20Dives/AWS%20STS/#sts-api-operations","title":"STS API Operations","text":""},{"location":"AWS%20Deep%20Dives/AWS%20STS/#assumerole","title":"<code>AssumeRole</code>","text":"<p>The AssumeRole API operation lets an IAM user assume an IAM role belonging to your account or to an external one (cross-account access). Once the request is successful, AWS generates and returns temporary credentials consisting of an access key ID, a secret access key, and a security token. These credentials can then be used by the IAM user to make requests to AWS services.</p>"},{"location":"AWS%20Deep%20Dives/AWS%20STS/#assumerolewithwebidentity","title":"<code>AssumeRoleWithWebIdentity</code>","text":"<p>The AssumeRoleWithWebIdentity API operation returns temporary security credentials for federated users who are authenticated through a public identity provider (e.g., Amazon Cognito, Login with Amazon, Facebook, Google, or any OpenID Connect-compatible identity provider). The temporary credentials can then be used by your application to establish a session with AWS. Just like the <code>AssumeRole</code> API, trusted entities who will be assuming the role must be specified. This time, instead of IAM users, it\u2019ll be an identity provider.</p> <p><code>AssumeRoleWithWebIdentity</code> does not require IAM Identities credentials, making it suitable for mobile applications that require access to AWS. The <code>AssumeRoleWithWebIdentity</code> is one of the APIs that Amazon Cognito uses under the hood to facilitate the exchange of token and credentials on your behalf. Because Amazon Cognito abstracts the hassles associated with user authentication, it is recommended that you use Amazon Cognito when providing AWS access to application users. However, you may just use <code>AssumeRoleWithWebIdentity</code> as a standalone operation.</p>"},{"location":"AWS%20Deep%20Dives/AWS%20STS/#assumerolewithsaml","title":"<code>AssumeRoleWithSAML</code>","text":"<p>The <code>AssumeRoleWithSAML</code> API operation returns a set of temporary security credentials for federated users who are authenticated by enterprise Identity Providers compatible with SAML 2.0. The users must also use SAML 2.0 (Security Assertion Markup Language) to pass authentication and authorization information to AWS. This API operation is useful for organizations that have integrated their identity systems (such as Windows Active Directory or OpenLDAP) with software that can produce SAML assertions.</p>"},{"location":"AWS%20Deep%20Dives/AWS%20STS/#getfederationtoken","title":"<code>GetFederationToken</code>","text":"<p>The GetFederationToken API operation returns a set of temporary security credentials consisting of a security token, access key, secret key, and expiration for a federated user. This API is usually used for federating access to users authenticated by a custom identity broker and can only be called using the programmatic credentials of an IAM user (IAM Roles are not supported). Although you can use the security credentials of a root user to call <code>GetFederationToken</code>, it is not recommended for security reasons. Instead, AWS advises creating an IAM user specifically for a proxy application that does the authentication process. The default expiration period for this API is significantly longer than <code>AssumeRole</code> (12 hours instead of one hour). Since you do not need to obtain new credentials as frequently, the longer expiration period can help reduce the number of calls to AWS.</p> <p>You can use the temporary credentials created by <code>GetFederationToken</code> in any AWS service except the following:</p> <ul> <li>You cannot call any IAM operations using the AWS CLI or the AWS API.</li> <li>You cannot call any STS operations except GetCallerIdentity .</li> </ul>"},{"location":"AWS%20Deep%20Dives/AWS%20STS/#decodeauthorizationmessage","title":"<code>DecodeAuthorizationMessage</code>","text":"<p><code>DecodeAuthorizationMessage</code> decodes additional information about the authorization status of a request from an encoded message returned in response to an AWS request. For example, a user might call an API to which he or she does not have access; the request results in a Client.UnauthorizedOperation response. Some AWS operations additionally return an encoded message that can provide details about this authorization failure. The message is encoded, so that privilege details about the authorization are hidden from the user who requested the operation. To decode an authorization status message, one must be granted permission via an IAM policy to request the <code>DecodeAuthorizationMessage</code> action.</p> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/STS/latest/APIReference/API_Operations.html</li> <li>https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html</li> </ul>"},{"location":"AWS%20Deep%20Dives/Amazon%20API%20Gateway/","title":"Amazon API Gateway\u202c","text":"<p>\u202dAmazon API Gateway is a fully managed service that allows you to publish, maintain, monitor, and secure your RESTful APIs. It serves as the entry point for your back-end services that are powered by AWS Lambda, Amazon EC2, Amazon ECS, AWS Elastic Beanstalk, or any web application. </p> <p>The following diagram illustrates how a request flows through API Gateway when an API is called.</p> <p></p> <ol> <li> <p><code>Method Request</code></p> <ul> <li>This section is where client requests are validated. Here, you can set up the authorization type (<code>AWS_IAM</code>, <code>NONE</code>, Lambda authorizer, Cognito authorizer) to control API access, enable the usage of API keys, or set up a request body validator using a Lambda function.</li> <li>You can also declare in the Method Request any input body, query string parameters, and HTTP headers that your API can accept.</li> </ul> </li> <li> <p><code>Integration Request</code></p> <ul> <li>The Integration Request section contains settings about how API Gateway communicates with the backend of your choosing (Lambda function, HTTP endpoint, Mock, AWS Service, VPC Link) and the integration type (proxy or non-proxy) API Gateway uses.</li> <li>For non-proxy integration, you have the option the use mapping templates to model the structure of the request data that gets forwarded to the backend.</li> </ul> </li> <li> <p><code>Integration Response</code></p> <ul> <li>This section only applies to a non-proxy integration. The Integration response intercepts the result returned from the backend before it\u2019s returned to the client.</li> <li>You must configure at least one integration response. The default response is Passthrough, which instructs API Gateway to return the response as-is. You may also transform the response to another format (base64 or text).</li> <li>Similarly to the Integration request, you have the option to transform the response data before it is returned to the client.</li> </ul> </li> <li> <p>Method Response</p> <ul> <li>Like Method Request, the Method Response is where you can define which HTTP headers the method can return.</li> </ul> </li> </ol>"},{"location":"AWS%20Deep%20Dives/Amazon%20API%20Gateway/#rest-api-vs-http-api-vs-websocket-api","title":"REST API vs. HTTP API vs. Websocket API","text":"<p>When you create an API, you get to choose between a REST API, HTTP API, and a WebSocket API endpoint.</p> <ul> <li>REST API<ul> <li>For standard use cases, the REST API is what you\u2019ll want to use.</li> <li>This gives you complete control over the request and response along with other API management capabilities like caching, creating API keys, and usage plans.</li> </ul> </li> <li>HTTP API<ul> <li>Cheaper than REST API</li> <li>Designed for simple applications</li> <li>Lacks other API Gateway features</li> </ul> </li> <li>WebSocket API<ul> <li>Typically used for real-time applications (e.g., chat applications, market trading applications)</li> </ul> </li> </ul> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html</li> <li>https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-basic-concept.html</li> </ul>"},{"location":"AWS%20Deep%20Dives/Amazon%20API%20Gateway/#proxy-vs-non-proxy-integration","title":"Proxy vs. Non-proxy integration\u202c","text":"<p>\u202dIn a\u202c\u202d Proxy integration, client\u2019s request is transmitted\u202c\u202d as is to the backend, including any headers or query\u202c parameters. No modification is done to the request data. As for the response, your backend is responsible for\u202c returning the response\u2019s status code, headers, and the payload to the client.\u202c</p> <p>In a\u202c\u202d Non-Proxy integration\u202c\u202d,\u202c\u202d API Gateway has control\u202c\u202d over how client data is formatted before it\u2019s passed down\u202c to your integration backend or before it\u2019s returned to the client. For example, instead of feeding the entire\u202c request data to your backend, you can filter it first using mapping templates at the\u202c\u202d <code>Integration</code> Request\u202c\u202d level \u202cto get only the portion that care for. The Non-Proxy integration is a bit more complex to implement and requires you to have knowledge of the Apached Velocity Template Language (VTL), which is the engine that API Gateway uses for mapping templates.</p> <p>Reference:</p> <ul> <li>https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html</li> </ul>"},{"location":"AWS%20Deep%20Dives/Amazon%20API%20Gateway/#stage-variables","title":"Stage variables\u202d","text":"<p>A Stage variable is a key-pair value that you can associate with a deployment stage of a REST API. You can\u202c think of them as environment variables where you store different parameters or configuration values that your\u202c API can access at runtime.\u202c</p> <p>Doing canary releases within the same REST API stage is a great application of stage variables. Canary is a\u202c deployment approach in which tra\ufb03c is split into two parts, one of which carries a larger portion of the tra\ufb03c\u202c and is routed to the production version, while the smaller portion points to the environment to which the new\u202c version is deployed. It can be a 90/10 split, 70/30 split, or even a 50/50 split. It all depends on how aggressive\u202c you want your deployment to be. This kind of setup allows developers to expose new features or bug fixes to a\u202c subset of users and receive feedback from them without having to shut down or make direct changes to the\u202c production version.\u202c</p> <p>\u202dConsider a REST API stage with a Lambda function as the backend. Assume you're about to release a new\u202c version of your API and want to test it with a subset of your users while continuing to serve the majority of\u202c tra\ufb03c with the old version. You can shift all tra\ufb03c to the new version once you\u2019re satisfied with it.\u202c\u202d As shown in\u202c the diagram below, the goal is to have a single API endpoint that dynamically interacts with two distinct\u202c versions of the Lambda function.\u202c</p> <p></p> <p>This is where stage variables come into play. In the example, we have a Lambda function with two aliases (prod and beta). Rather than writing the actual aliases in your integration backend's settings, we can use the stage variable ver as a placeholder. You can then switch between different values of ver in the Canary setting of the API Stage and control the amount of tra\ufb03c that goes to different aliases.</p> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</li> <li>https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html</li> </ul>"},{"location":"AWS%20Deep%20Dives/Amazon%20API%20Gateway/#mapping-templates","title":"Mapping Templates","text":"<p>Mapping templates allow you to modify any request data before it is forwarded to your integration backend. Conversely, it can be used for transforming the response data before it is returned to the client. Let\u2019s further understand how this works with an example.</p> <p>Say you\u2019ve built an API for a music application that returns various details about an artist. To keep it simple, consider the following JSON data:\u202d</p> <pre><code>{\n    \"id\": 1234,\n    \"artist\": \"Queen\",\n    \"popularity\": 90,\n    \"genres\": [\n        \"Progressive rock\",\n        \"Pop rock\",\n        \"Glam rock\"\n    ]\n}\n</code></pre> <p>If a user wants to obtain the popularity score or the genre of a certain artist, there\u2019s no way for him/her to retrieve the information that he/she only cares for. To solve this, you can use mapping templates to modify the response before it is sent back to the user. Assuming that your API has a resource path named <code>/{id}</code> , a child resource path called <code>/genres</code> can be created under it. In the integration response settings, add a mapping template to construct the new response.</p> <p>After redeploying the API, users can now send <code>GET</code> requests to the <code>/{id}/genres</code> resource. In our example, the response would simply be:</p> <pre><code>{\n    \"genre\": [\n        \"Progressive rock\",\n        \"Pop rock\",\n        \"Glam rock\"\n    ]\n}\n</code></pre> <p>Mapping templates are written in the Velocity Template Language or VTL \u2014 a Java-based template engine developed by Apache. If you aren\u2019t familiar with it, getting comfortable with mapping templates may take you some time. Don\u2019t worry, as knowledge of VTL is not required in the CDA exam; you must only understand what a mapping template is and what it is used for at a high level.</p> <p>Keep in mind that mapping templates only work for non-proxy integrations . This is because in proxy integrations, API Gateway simply passes the data it receives to both ends and is unaware of how the request/response is modeled.</p> <p>You can also use mapping templates to modernize legacy applications. If you have a legacy application that\u202d you want to expose, say a SOAP web service that processes XML data, you can have clients send\u202c JSON-formatted requests and then have Amazon API Gateway transform that JSON data to XML using\u202d mapping templates. As for the integration backend, you can use a Lambda function as middleware to transmit\u202c the XML as a payload to the SOAP web service.\u202c</p>"},{"location":"AWS%20Deep%20Dives/Amazon%20API%20Gateway/#invalidating-cache","title":"Invalidating Cache\u202c","text":"<p>Caching often leads to data inconsistency, which is a problem commonly faced. When content is cached, API\u202c Gateway does not update the cache entries until the Time-To-Live (TTL) expires. As a result, any changes made\u202c to the database will not immediately reflect on the client-side, leading to a disparity between the actual content\u202c and what is displayed on the application. However, you can take steps to mitigate this issue by sending an\u202c invalidation request to your API endpoint. This will prompt API Gateway to refresh its cache instead of waiting\u202c for the TTL to expire.\u202c</p> <p>To invalidate a cache entry, simply include the\u202c\u202d <code>Cache-Control\u202c\u202d</code> header in a request with a\u202c\u202d <code>max-age\u202c\u202d</code> of <code>0</code>, as\u202c\u202d shown in the example below that uses the Fetch API in Javascript.</p> <pre><code>fetch('https://aaaa4vb5wf.execute-api.us-east-1.amazonaws.com/v1', {\n    method: 'GET',\n    headers : {\n      'Cache-Control': 'max-age=0'\n    }\n  });\n</code></pre> <p>Reference:</p> <ul> <li>https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching</li> </ul>"},{"location":"AWS%20Deep%20Dives/Amazon%20API%20Gateway/#cross-origin-resource-sharing-cors","title":"Cross-Origin Resource Sharing (CORS)","text":"<p>CORS a security mechanism that most web browsers such as Google Chrome or Mozilla Firefox enforce to relax the restrictions of the same-origin policy. The same-origin policy is a browser security feature that limits scripts loaded from an origin to only interact with resources from the same origin. While the intention is good, sometimes it can be too restrictive. Businesses today usually rely on third-party APIs to quickly add features to their applications. This would not be possible with the Same-Origin Policy in place. To solve this problem, engineers came up with the idea of Cross-origin resource sharing to loosen up the Same-Origin Policy restrictions.</p> <p></p> <p>Say you visit a website called <code>pet.com</code> using Google Chrome. Upon loading, Google Chrome will download the required assets (HTML, Javascript, images, fonts, etc.) from the website\u2019s server to render the webpage. As you browse the website, you come across a fun feature that uses Artificial Intelligence (AI) to identify dog or cat breeds based on an image you upload. This feature is powered by a third-party API (<code>petbreed.com</code>), which is accessed via a Javascript file. Google Chrome will not immediately send a GET request to <code>petbreed.com</code> after you submit an image. Instead, it will first send a preflight OPTIONS request to confirm if <code>petbreed.com</code> does indeed allow <code>pet.com</code> to make GET requests. At the <code>petbreed.com</code> server\u2019s end, the allowed domains and methods can be specified through the <code>Access-Control-Allow-Origin</code> and the <code>Access-Control-Request-Method</code> headers. The API developer can select which values to put in those headers. For example, <code>pet.com</code> can be defined as an <code>Access-Control-Allow-Origin</code> header value and <code>GET</code> as an <code>Access-Control-Request-Method</code> header value. This will explicitly grant <code>pet.com</code> to make <code>GET</code> requests to <code>petbreed.com</code>. Next, <code>petbreed.com</code> returns the list of allowed API methods and domains to Google Chrome. If <code>pet.com</code> is specified in the <code>Access-Control-Allow-Origin</code>, only then will Google Chrome send the actual <code>GET</code> request.</p> <p>Just like <code>petbreed.com</code>, you can also configure CORS in API Gateway. Please note that CORS is disabled by default. CORS is configured at the resource method level when using non-proxy integrations. You must specify the proper access control headers in the header mappings of your API's integration response. On the other hand, if you\u2019re using a proxy integration, you must explicitly declare the access control headers in the response returned by your backend.</p> <p>References:</p> <ul> <li>https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html</li> <li>https://aws.amazon.com/blogs/compute/configuring-cors-on-amazon-api-gateway-apis/</li> </ul>"},{"location":"AWS%20Deep%20Dives/Amazon%20API%20Gateway/#authorizers","title":"Authorizers","text":"<p>API Gateway lets you use Cognito User Pool or a Lambda function to authorize client requests.</p> <p>How Cognito User Pool authorizer works:</p> <p></p> <ol> <li>When a user logs in to the User Pool, Cognito checks if the credentials the user has submitted are valid.</li> <li>If the login is successful, Cognito returns a JSON web token (JWT) to the client.</li> <li>The JSON web token (JWT) is passed to a custom header which is included as part of the API request.</li> <li>API Gateway will look for the custom header and verify its validity from Amazon Cognito.</li> <li>Once verified, API Gateway accepts the request and performs the API call. In this case, the Lambda function, which is the backend, is invoked. If there are no issues, API Gateway returns a 200 status code along with the response from the Lambda function back to the client.</li> </ol> <p>You might want to implement a Lambda Function Authorizer to enforce custom authorization logic, such as those that employ bearer token authentication strategies (OAuth) or something that uses request parameters to determine the caller's identity. Since this is a custom method, you have to write the logic that carries out the authorization process. As a result, this takes more development effort on your end compared to using the Cognito User Pool.</p> <p>How a Lambda function Authorizer works:</p> <p></p> <ol> <li>The application sends a GET method to API Gateway, along with a bearer token or request parameters.</li> <li>API Gateway will check whether a Lambda authorizer is enabled for the method. If it is, API Gateway calls the Lambda function that authorizes the request.</li> <li>The Lambda function authenticates the request. If the call succeeds, the Lambda function grants access by returning an output object containing at least an IAM policy and a principal identifier.</li> <li>API Gateway evaluates the policy. If access is denied, API Gateway returns a 403 forbidden status code and If access is allowed, API Gateway performs the GET request.</li> </ol> <p>Reference:</p> <ul> <li>https://aws.amazon.com/blogs/compute/introducing-custom-authorizers-in-amazon-api-gateway/</li> </ul>"},{"location":"AWS%20Deep%20Dives/Amazon%20API%20Gateway/#usage-plans","title":"Usage Plans","text":"<p>If you have an idea for an API that you\u2019d like to expose or sell, perhaps you\u2019ve built an AI service that can help out other businesses, then you might want to look at Usage Plans. Usage plans is an API Gateway feature that can help you control different levels of access to an API. For example, you can create three subscription plans for your API: basic, standard, and premium plan. A usage plan can be used to set distinct throttling and quota limitations based on a subscription, which is then enforced on specific client API keys.</p> <p>Aside from that, you can sell your API as a product in the AWS SaaS Marketplace so other users or companies can see your product and subscribe to it. Your API subscribers are billed based on the number of requests made to the usage plan.</p> <p>References:</p> <ul> <li>https://aws.amazon.com/blogs/aws/new-usage-plans-for-amazon-api-gateway/</li> <li>https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html</li> </ul>"}]}